{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import operator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we're only looking at the first set of tweets.\n",
    "# Get Data from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_tweets(start, end = False):\n",
    "    data = []\n",
    "    if end == False:\n",
    "        end = start\n",
    "    for i in range(start, end + 1):\n",
    "        csv_file = open('all_tweets/IRAhandle_tweets_' + str(i) + '.csv')\n",
    "        data += list(csv.DictReader(csv_file))\n",
    "        csv_file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alldata = open_tweets(1,13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243891\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the domains that these tweets are linking to.\n",
    "# Get Domain Info & Sort Into All / Russian / Non-Russian\n",
    "##### Format of each dictionary:\n",
    "\n",
    "{(string) *domain_name*: (array) [(int) *count*, (array) *tweet_id's* linking to this domain]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_categories = ['tco1_step1', 'tco2_step1', 'tco3_step1']\n",
    "user_info = ['author']\n",
    "details = ['retweet', 'account_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.youtu.be/\n"
     ]
    }
   ],
   "source": [
    "domain_starters = ['http://', 'https://']\n",
    "common_bits = ['www.', '.com']\n",
    "\n",
    "# UNUSED AS OF NOW\n",
    "def get_domain_old(link, no_common = False):\n",
    "    # removes the http:// and https://, and does not have the ending slash\n",
    "    longest_starter_len = max([len(i) for i in domain_starters])\n",
    "    longest_common_len = max([len(i) for i in common_bits])\n",
    "    for ds in domain_starters:\n",
    "        if ds in link[:longest_starter_len]:\n",
    "            beginning = link[len(ds):].split('/')[0]\n",
    "            if no_common:\n",
    "                for b in common_bits:\n",
    "                    if b in beginning:\n",
    "                        beginning = [r for r in beginning.split(b) if r != ''][0] \n",
    "            return beginning.lower()\n",
    "\n",
    "def get_domain(link, no_common = False):\n",
    "    longest_common_len = max([len(i) for i in common_bits])\n",
    "    beginning = '/'.join(link.split('/')[:3]) # we assume we have something like pre://domain_name/...\n",
    "    if no_common:\n",
    "        for b in common_bits:\n",
    "            if b in beginning:\n",
    "                beginning = [r for r in beginning.split(b) if r != ''][0]\n",
    "    return beginning.lower() + '/'\n",
    "        \n",
    "def basic_dict_checks(dct, key, twt_id):\n",
    "    if key in dct.keys():\n",
    "        dct[key][COUNT_IND] += 1\n",
    "        dct[key][TWT_ID_IND] += [twt_id]\n",
    "    else:\n",
    "        dct[key] = []\n",
    "        dct[key] += [1]\n",
    "        dct[key] += [[twt_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_domains = {}\n",
    "russian_sites = {}\n",
    "non_russian_sites = {}\n",
    "\n",
    "COUNT_IND = 0\n",
    "TWT_ID_IND = 1\n",
    "\n",
    "total_links = 0\n",
    "\n",
    "for tweet in data:\n",
    "    tweet_id = tweet['tweet_id']\n",
    "    for link in link_categories:\n",
    "        dom = get_domain(tweet[link])\n",
    "        if dom and dom != '/':\n",
    "            basic_dict_checks(unique_domains, dom, tweet_id)\n",
    "            if '.ru' in dom:\n",
    "                basic_dict_checks(russian_sites, dom, tweet_id)\n",
    "            else:\n",
    "                basic_dict_checks(non_russian_sites, dom, tweet_id)\n",
    "            total_links += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the Domains (now stored in lists) by Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_domains = sorted(unique_domains.items(), key=lambda k: k[1][0], reverse=True)\n",
    "sorted_russian_sites = sorted(russian_sites.items(), key=lambda k: k[1][0], reverse=True)\n",
    "sorted_non_russian_sites = sorted(non_russian_sites.items(), key=lambda k: k[1][0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def domain_basics(site_info):\n",
    "    # given a site: info pair, returns an array with the domain name and number of tweets linking to that domain\n",
    "    return [site_info[0], site_info[1][COUNT_IND]]\n",
    "def print_title(title):\n",
    "    print(title)\n",
    "    print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Referenced Sites in Russian Tweets (Ignoring Twitter)\n",
      "========================================\n",
      "['http://ift.tt/', 24008]\n",
      "\n",
      "RUSSIAN: ['http://www.gazeta.ru/', 7728]\n",
      "\n",
      "['http://bit.ly/', 7552]\n",
      "['http://dlvr.it/', 3794]\n",
      "\n",
      "RUSSIAN: ['http://ria.ru/', 3548]\n",
      "\n",
      "['https://twibble.io/', 3427]\n",
      "['http://twib.in/', 3122]\n",
      "\n",
      "RUSSIAN: ['http://tass.ru/', 2921]\n",
      "\n",
      "['http://www.fox5atlanta.com/', 2706]\n",
      "['http://ow.ly/', 2349]\n",
      "['http://usfreedomarmy.com/', 2070]\n",
      "['http://fb.me/', 1668]\n",
      "['http://www.kob.com/', 1615]\n",
      "['https://www.youtube.com/', 1494]\n",
      "['https://youtu.be/', 1258]\n",
      "['https://goo.gl/', 1256]\n",
      "['https://www.instagram.com/', 1220]\n",
      "['http://krqe.com/', 1152]\n",
      "\n",
      "RUSSIAN: ['http://www.rbc.ru/', 1128]\n",
      "\n",
      "['http://dld.bz/', 965]\n"
     ]
    }
   ],
   "source": [
    "print_title(\"Top 20 Referenced Sites in Russian Tweets (Ignoring Twitter)\")\n",
    "for site in sorted_domains[1:21]:\n",
    "    if '.ru' in site[0]:\n",
    "        print('\\nRUSSIAN: ' + str(domain_basics(site)) + '\\n')\n",
    "    else:\n",
    "        print(domain_basics(site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Russian Sites in Russian Tweets\n",
      "========================================\n",
      "['http://www.gazeta.ru/', 7728]\n",
      "['http://ria.ru/', 3548]\n",
      "['http://tass.ru/', 2921]\n",
      "['http://www.rbc.ru/', 1128]\n",
      "['http://www.postsovet.ru/', 942]\n",
      "['http://riafan.ru/', 561]\n",
      "['http://r.rbc.ru/', 486]\n",
      "['http://rsport.ru/', 413]\n",
      "['https://ria.ru/', 397]\n",
      "['http://vesti.ru/', 323]\n",
      "['http://www.vesti.ru/', 310]\n",
      "['http://lifenews.ru/', 291]\n",
      "['https://tvrain.ru/', 286]\n",
      "['http://www.kp.ru/', 268]\n",
      "['http://izvestia.ru/', 244]\n",
      "['http://top.rbc.ru/', 210]\n",
      "['http://vz.ru/', 201]\n",
      "['http://tvzvezda.ru/', 200]\n",
      "['http://go.tass.ru/', 194]\n",
      "['http://www.ntv.ru/', 148]\n"
     ]
    }
   ],
   "source": [
    "print_title(\"Top 20 Russian Sites in Russian Tweets\")\n",
    "for site in sorted_russian_sites[:20]:\n",
    "    print(domain_basics(site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Non-Russian Sites in Russian Tweets\n",
      "========================================\n",
      "['https://twitter.com/', 121457]\n",
      "['http://ift.tt/', 24008]\n",
      "['http://bit.ly/', 7552]\n",
      "['http://dlvr.it/', 3794]\n",
      "['https://twibble.io/', 3427]\n",
      "['http://twib.in/', 3122]\n",
      "['http://www.fox5atlanta.com/', 2706]\n",
      "['http://ow.ly/', 2349]\n",
      "['http://usfreedomarmy.com/', 2070]\n",
      "['http://fb.me/', 1668]\n",
      "['http://www.kob.com/', 1615]\n",
      "['https://www.youtube.com/', 1494]\n",
      "['https://youtu.be/', 1258]\n",
      "['https://goo.gl/', 1256]\n",
      "['https://www.instagram.com/', 1220]\n",
      "['http://krqe.com/', 1152]\n",
      "['http://dld.bz/', 965]\n",
      "['https://russian.rt.com/', 943]\n",
      "['http://ind.pn/', 904]\n",
      "['http://larep.it/', 843]\n"
     ]
    }
   ],
   "source": [
    "print_title(\"Top 20 Non-Russian Sites in Russian Tweets\")\n",
    "for site in sorted_non_russian_sites[:20]:\n",
    "    print(domain_basics(site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of Unique Domains:\n",
      "========================================\n",
      "Russian: 479\n",
      "Non-Russian: 5756\n",
      "for a total of 6235 unique domains for 243891 tweets\n",
      "percentage of unique Russian sites over all: 7.68%\n",
      "\n",
      "Number of Links to Domains:\n",
      "========================================\n",
      "Russian: 23240\n",
      "Non-Russian: 226515\n",
      "for a total of 249755 links in 243891 tweets\n",
      "percentage of Russian links over all: 9.31%\n"
     ]
    }
   ],
   "source": [
    "t_russites = len(sorted_russian_sites)\n",
    "t_norussites = len(sorted_non_russian_sites)\n",
    "t_sites = len(sorted_domains)\n",
    "total_tweets = len(data)\n",
    "\n",
    "t_ruslinks = sum([deet[0] for (name, deet) in sorted_russian_sites])\n",
    "t_noruslinks = sum([deet[0] for (name, deet) in sorted_non_russian_sites])\n",
    "\n",
    "print_title(\"Numbers of Unique Domains:\")\n",
    "print('Russian: ' + str(t_russites))\n",
    "print('Non-Russian: ' + str(t_norussites))\n",
    "print('for a total of ' + str(t_sites) + ' unique domains for ' + str(total_tweets) + ' tweets')\n",
    "print('percentage of unique Russian sites over all: ' + str(round(t_russites / t_sites * 100, 2)) + '%')\n",
    "\n",
    "print_title(\"\\nNumber of Links to Domains:\")\n",
    "print('Russian: ' + str(t_ruslinks))\n",
    "print('Non-Russian: ' + str(t_noruslinks))\n",
    "print('for a total of ' + str(total_links) + ' links in ' + str(total_tweets) + ' tweets')\n",
    "print('percentage of Russian links over all: ' + str(round(t_ruslinks / total_links * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Preliminary Observations:\n",
    "### Some automation websites\n",
    "dlvr.it, twibble.io + twib.in, ift.tt (?)\n",
    "### A lot of shortening websites\n",
    "tinyurl, bit.ly, ow.ly, goo.gl, fb.me\n",
    "### A few repeated websites w/ 'different' domains\n",
    "youtu.be vs youtube.com, rbc.com\n",
    "### Top sites are:\n",
    "shortening websites, automation websites, social media, Fox news, USFreedomArmy; RUSSIATODAY (not included in Russian websites right now)\n",
    "# NEXT question: matching with our categories?\n",
    "## NOTE: this is all without having figured out all shortlink websites thus far; numbers may (?) go up afterwards. Also, haven't quite figured out what to do with the Twitter links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_match_sites(types):\n",
    "    # takes a list of category names\n",
    "    sites = []\n",
    "    for name in types:\n",
    "        sites += [i.rstrip('\\n') for i in open('categories/' + name).readlines()]\n",
    "    matching = []\n",
    "    for site in sites:\n",
    "        if site in unique_domains.keys():\n",
    "            matching += [(site, unique_domains[site][0])]\n",
    "    return matching\n",
    "\n",
    "def total_matches(matches):\n",
    "    return sum([num for name, num in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fake_matches = read_and_match_sites(['fake-news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Matching Fake News Sites & Counts\n",
      "========================================\n",
      "('http://www.thegatewaypundit.com/', 307)\n",
      "('http://conservativetribune.com/', 122)\n",
      "('http://bb4sp.com/', 58)\n",
      "('http://www.dcclothesline.com/', 49)\n",
      "('http://beforeitsnews.com/', 43)\n",
      "('http://eaglerising.com/', 43)\n",
      "('http://100percentfedup.com/', 31)\n",
      "('https://www.jihadwatch.org/', 27)\n",
      "('http://www.truthrevolt.org/', 22)\n",
      "('https://conservativedailypost.com/', 20)\n",
      "('http://conservativefighters.com/', 20)\n",
      "('http://pamelageller.com/', 19)\n",
      "('http://rickwells.us/', 15)\n",
      "('http://www.frontpagemag.com/', 15)\n",
      "('http://freedomdaily.com/', 13)\n",
      "('http://endingthefed.com/', 11)\n",
      "('http://www.israelvideonetwork.com/', 11)\n",
      "('http://www.barenakedislam.com/', 10)\n",
      "('http://freedomoutpost.com/', 10)\n",
      "('http://allenwestrepublic.com/', 8)\n",
      "\n",
      "Total Fake link matches: 1034 out of 249755 total links\n",
      "Percentage fake over all: 0.414 %\n"
     ]
    }
   ],
   "source": [
    "print_title('Top 20 Matching Fake News Sites & Counts')\n",
    "for i in sorted(fake_matches, key = operator.itemgetter(1), reverse=True)[:20]:\n",
    "    print(i)\n",
    "print('\\nTotal Fake link matches: ' + str(total_matches(fake_matches)) + ' out of ' + str(total_links) + ' total links')\n",
    "print('Percentage fake over all: ' + str(round(total_matches(fake_matches) / total_links * 100, 3)) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conspiracy_matches = read_and_match_sites(['conspiracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Matching Conspiracy Sites & Counts\n",
      "========================================\n",
      "('http://www.zerohedge.com/', 212)\n",
      "('http://www.infowars.com/', 116)\n",
      "('http://truepundit.com/', 71)\n",
      "('http://worldtruth.tv/', 29)\n",
      "('http://consciouslifenews.com/', 21)\n",
      "('http://www.disclose.tv/', 21)\n",
      "('http://yournewswire.com/', 20)\n",
      "('http://21stcenturywire.com/', 18)\n",
      "('http://countercurrentnews.com/', 17)\n",
      "('http://www.veteranstoday.com/', 14)\n",
      "('http://shoebat.com/', 12)\n",
      "('http://www.activistpost.com/', 11)\n",
      "('http://www.nowtheendbegins.com/', 10)\n",
      "('http://www.ecowatch.com/', 7)\n",
      "('http://www.globalresearch.ca/', 7)\n",
      "('http://wearechange.org/', 7)\n",
      "('http://theantimedia.org/', 7)\n",
      "('http://www.coasttocoastam.com/', 6)\n",
      "('https://www.sott.net/', 6)\n",
      "('http://www.centerforsecuritypolicy.org/', 5)\n",
      "\n",
      "Total Conspiracy link matches: 675 out of 249755 total_links\n",
      "Percentage conspiracy over all: 0.27 %\n"
     ]
    }
   ],
   "source": [
    "print_title('Top 20 Matching Conspiracy Sites & Counts')\n",
    "for i in sorted(conspiracy_matches, key = operator.itemgetter(1), reverse=True)[:20]:\n",
    "    print(i)\n",
    "print('\\nTotal Conspiracy link matches: ' + str(total_matches(conspiracy_matches)) + ' out of ' + str(total_links) + ' total_links')\n",
    "print('Percentage conspiracy over all: ' + str(round(total_matches(conspiracy_matches) / total_links * 100, 3)) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_matches = read_and_match_sites(['center', 'conspiracy', 'fake-news', \n",
    "                                           'left', 'left-center', 'pro-science',\n",
    "                                          'right', 'right-center'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Matching MediaBiasFactCheck Sites & Counts\n",
      "========================================\n",
      "('http://www.breitbart.com/', 513)\n",
      "('http://www.thegatewaypundit.com/', 307)\n",
      "('http://www.ajc.com/', 268)\n",
      "('http://www.foxnews.com/', 261)\n",
      "('http://dailycaller.com/', 230)\n",
      "('http://www.zerohedge.com/', 212)\n",
      "('http://www.huffingtonpost.com/', 128)\n",
      "('http://www.wnd.com/', 125)\n",
      "('http://conservativetribune.com/', 122)\n",
      "('http://www.infowars.com/', 116)\n",
      "('https://www.washingtonpost.com/', 115)\n",
      "('http://thehill.com/', 100)\n",
      "('http://www.bizpacreview.com/', 100)\n",
      "('http://twitchy.com/', 95)\n",
      "('http://www.washingtonexaminer.com/', 93)\n",
      "('http://therightscoop.com/', 72)\n",
      "('http://truepundit.com/', 71)\n",
      "('http://www.politico.com/', 69)\n",
      "('http://www.theblaze.com/', 66)\n",
      "('http://freebeacon.com/', 66)\n",
      "\n",
      "Total MBFC link matches: 7198 out of 249755 total_links\n",
      "Percentage MBFC over all: 2.882 %\n"
     ]
    }
   ],
   "source": [
    "print_title('Top 20 Matching MediaBiasFactCheck Sites & Counts')\n",
    "for i in sorted(all_matches, key = operator.itemgetter(1), reverse=True)[:20]:\n",
    "    print(i)\n",
    "print('\\nTotal MBFC link matches: ' + str(total_matches(all_matches)) + ' out of ' + str(total_links) + ' total_links')\n",
    "print('Percentage MBFC over all: ' + str(round(total_matches(all_matches) / total_links * 100, 3)) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domains that don't match MBFC: 5168\n",
      "Total domains in Russian tweets: 5756\n",
      "Total domains that match MBFC: 590 (why doesn't this add up?)\n"
     ]
    }
   ],
   "source": [
    "all_matches_domains = [name for name, numbers in all_matches]\n",
    "non_matches = [name for name, info in sorted_non_russian_sites if name not in all_matches_domains]\n",
    "print(\"Domains that don't match MBFC: \" + str(len(non_matches)))\n",
    "print(\"Total domains in Russian tweets: \" + str(len(sorted_non_russian_sites)))\n",
    "print(\"Total domains that match MBFC: \" + str(len(all_matches_domains)) + \" (why doesn't this add up?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange that it doesn't add up exactly... Need to check that later. In the interim, let's see what the 'non-matches' are like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://reagangirl.com/', 'http://blackpyramidclothing.com/', 'http://www.foxbusiness.com/', 'http://dv.land/', 'http://www.almanar.com.lb/', 'http://mk.gov/', 'http://www.krownher.com/', 'http://vote.peopleschoice.com/', 'https://usat.ly/', 'http://www.thedreamcarevent.com/', 'http://solar-cells.beforeitsnews.com/', 'https://syria.liveuamap.com/', 'http://whattheredheadsaid.com/', 'http://topru.org/', 'https://pro.hsionlineorders.net/', 'http://www.hallmarkintergroup.com/', 'http://thkpr.gs/', 'http://projectpurge.com/', 'http://www.thechampaignroom.com/', 'http://www.rivoli.ca/']\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(non_matches, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like... some subdomains might be a problem, as we thought before. Also branches of some popular sites... though also, there seem to be some new interesting websites for us to consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTIONS FOR NEXT TIME:\n",
    "## Initial Stats\n",
    "1. How shall we deal with the twitter links?\n",
    "2. Which are the most prominent url shortening domains? We should figure out how those redirect.\n",
    "3. How to deal with \"same domains\" like youtube.com vs youtu.be?\n",
    "4. What do our stats look like if we take out the influence of the unknowns (twitter links) and shortening links?\n",
    "5. What are the sites that are NOT in our matching list??\n",
    "\n",
    "## Network Work\n",
    "1. Keep track of the individual accounts (can we?) and see who links what websites: if a user links to two different websites, we can add to the weight between those two websites.\n",
    "\n",
    "## Miscellaneous\n",
    "1. Saw some Italian/European websites in the mix? Might be interesting to see what's up with that.\n",
    "2. Also, there's some interesting stuff about what languages each tweet is in; maybe we can see if there's explictly Russian tweets related to more of the Russian sites?\n",
    "3. RussiaToday is totally a Russia-based site, but right now we're only using the '.ru' to check if it's a Russian site; do we need to do more work on that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
